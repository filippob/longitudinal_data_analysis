---
title: "cross validation"
author: "Filippo Biscarini"
date: "2023-06-21"
output: html_document
---


```{r setup, include=FALSE}
library("knitr")
library("xgboost")
library("tidyverse")
library("data.table")
library("tidymodels")

knitr::opts_chunk$set(echo = TRUE)
```

We introduce here resampling and cross-validation for predictive models in R.
A random cross-validation approach is used.

## Read data

Data from [Spatiotemporally explicit model averaging for forecasting of Alaskan groundfish catch](https://onlinelibrary.wiley.com/doi/10.1002/ece3.4488)
(data repo [here](https://zenodo.org/record/4987796#.ZHcLL9JBxhE))

It's data on fish catch (multiple fish species) over time in different regions of Alaska.

```{r read-the-data}
basefolder = "/home/filippo/Dropbox/cursos/longitudinal_data_analysis/longitudinal_data_analysis"
inpfile = "data/alaska_groundfish_catch/stema_data.csv"
fname = file.path(basefolder, inpfile)

fish = fread(fname)
```

-   **CPUE**: target variable, "catch per unit effort"
-   **SST**: sea surface temperature
-   **CV**: actually, the coefficient of variation for SST is used $\rightarrow$ the coefficient of variation is an improved measure of seasonal SST over the mean, because it standardizes scale and allows us to consider the changes in variation of SST with the changes in mean over (Hannah Correia, 2018 - Ecology and Evolution)
-   **SSTcvW1-5**: CPUE is influenced by survival in the first year of life. Water temperature affects survival, and juvenile fish are more susceptible to environmental changes than adults. Therefore, CPUE for a given year is likely linked to the winter SST at the juvenile state. Since this survey targets waters during the summer and the four species covered reach maturity at 5--8 years, SST was lagged for years one through five to allow us to capture the effect of SST on the juvenile stages. All five lagged SST measures were included for modeling.

### Data preprocessing

- `V1` is record ID
- `Station` indicates the fishing station

We will not consider these variables in the predictive model: remove here, or use `tidymodels` `roles`?

In order to accommodate variation in SST among stations, the CPUE value has been replicated multiple times. This would defeat our purpose of analysing data by group (fish species) over space and time: with only one value per group, a statistical analysis is a bit hard to be performed (no variation). Therefore, to the original CPUE values we add some random noise proportional to the average (by species, area, year):

```{r preprocess}
fish <- fish |>
  group_by(Species, Area, Year) |>
  mutate(avg = mean(CPUE), std = 0.1*avg)
```

```{r}
fish <- fish |>
  ungroup() |>
  mutate(noise = rnorm(n = n(),mean = 0,sd = std), CPUE = CPUE + noise)
```

```{r}
temp <- fish |>
  select(-c(V1,Station,avg,std,noise))
```


## Data splitting

The validation-set approach:

### 1. Define the data split

```{r}
splits      <- initial_split(temp, prop = 0.8)
fish_train <- training(splits)
fish_test  <- testing(splits)
```


### 2. Define the model

We use boosting (ensemble method) with hard-coded parameters:

```{r}
m = round((ncol(temp)-1)/3,0) ##(CPUE: target variable)
boost_mod <- 
  boost_tree(mode = "regression", mtry = m, trees = 1000, min_n = 5, tree_depth = 5, learn_rate = 0.1) %>% 
  set_engine("xgboost")
```

### 3. Define the recipe

We define the recipe: the model equation, the filtering (zero variance predictors), the dummy variables for categorical predictors:

```{r}
boost_recipe <- 
  recipe(CPUE ~ ., data = fish_train) %>% 
  update_role(CPUE, new_role = "outcome") |>
  step_zv(all_predictors()) %>% 
  # step_normalize(all_numeric_predictors()) |>
  step_dummy(all_nominal_predictors())
```

From the recipes, we can get the prepprocessed training (and testing) data:

```{r}
fish_prepped <- prep(boost_recipe)
bake(fish_prepped, new_data = fish_train)
```

### 4. Define the workflow

We bring together the model and the recipe in a single workflow:

```{r}
boost_workflow <- 
  workflow() %>% 
  add_model(boost_mod) %>% 
  add_recipe(boost_recipe)
```

### 5. Fit the model

We fit (train) the model on the training data:

```{r}
boost_fit <- 
  boost_workflow %>% 
  fit(data = fish_train)
```

### 6. Get model predictions and evaluate the model

```{r}
predict(boost_fit, fish_test)
```

```{r}
test_aug <- 
  augment(boost_fit, fish_test)
```

```{r}
ggplot(test_aug, aes(.pred, CPUE)) + geom_point()
```

```{r}
test_aug |>
  summarise(avg = mean(CPUE),
            accuracy = cor(.pred,CPUE),
            N = n(),
            RMSE = sqrt(sum((.pred - CPUE)^2)/N),
            NRMSE = RMSE/avg)
```

#### And in the training set?

```{r}
train_aug <- 
  augment(boost_fit, fish_train)
```

```{r}
ggplot(train_aug, aes(.pred, CPUE)) + geom_point()
```

```{r}
train_aug |>
  summarise(avg = mean(CPUE),
            accuracy = cor(.pred,CPUE),
            N = n(),
            RMSE = sqrt(sum((.pred - CPUE)^2)/N),
            NRMSE = RMSE/avg)
```

## Cross-validation

```{r}
folds <- vfold_cv(temp, v = 10)
folds
```

```{r}
boost_wf <- 
  workflow() %>%
  add_model(boost_mod) %>%
  add_recipe(boost_recipe)
```

```{r}
boost_fit_rs <- 
  boost_wf %>% 
  fit_resamples(folds, control = control_resamples(save_pred = TRUE))
```

```{r}
collect_metrics(boost_fit_rs)
```

```{r}
assess_res <- collect_predictions(boost_fit_rs)
res <- assess_res |>
  group_by(id) |>
    summarise(avg = mean(CPUE),
            accuracy = cor(.pred,CPUE),
            N = n(),
            RMSE = sqrt(sum((.pred - CPUE)^2)/N),
            NRMSE = RMSE/avg)

print(res)
```

And the average accuracy (Pearson correlation) and NRMSE over the 10 cross-validation replicates: 

```{r}
mean(res$accuracy)
```

```{r}
mean(res$NRMSE)
```

