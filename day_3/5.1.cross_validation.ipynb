{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cross validation\n",
        "\n",
        "We introduce here resampling and cross-validation for predictive models in R. A random cross-validation approach is used."
      ],
      "metadata": {
        "id": "tGqFxw_euVHj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAUIcXomuOCP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the data\n",
        "\n",
        "Data from [Spatiotemporally explicit model averaging for forecasting of Alaskan groundfish catch](https://onlinelibrary.wiley.com/doi/10.1002/ece3.4488) (data repo [here](https://zenodo.org/record/4987796#.ZHcLL9JBxhE))\n",
        "\n",
        "It's data on fish catch (multiple fish species) over time in different regions of Alaska."
      ],
      "metadata": {
        "id": "4GlljdBSufaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url= \"https://zenodo.org/records/4987796/files/stema_data.csv\"\n",
        "fish = pd.read_csv(url)\n",
        "fish.head()"
      ],
      "metadata": {
        "id": "1RNNXlgeugnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-   **CPUE**: target variable, \"catch per unit effort\"\n",
        "-   **SST**: sea surface temperature\n",
        "-   **CV**: actually, the coefficient of variation for SST is used $\\rightarrow$ the coefficient of variation is an improved measure of seasonal SST over the mean, because it standardizes scale and allows us to consider the changes in variation of SST with the changes in mean over (Hannah Correia, 2018 - Ecology and Evolution)\n",
        "-   **SSTcvW1-5**: CPUE is influenced by survival in the first year of life. Water temperature affects survival, and juvenile fish are more susceptible to environmental changes than adults. Therefore, CPUE for a given year is likely linked to the winter SST at the juvenile state. Since this survey targets waters during the summer and the four species covered reach maturity at 5--8 years, SST was lagged for years one through five to allow us to capture the effect of SST on the juvenile stages. All five lagged SST measures were included for modeling.\n",
        "\n",
        "### Data preprocessing\n",
        "\n",
        "-   `V1` is record ID\n",
        "-   `Station` indicates the fishing station\n",
        "\n",
        "We will not consider these variables in the predictive model: remove here, or use `tidymodels` `roles`?\n",
        "\n",
        "In order to accommodate variation in SST among stations, the CPUE value has been replicated multiple times. This would defeat our purpose of analysing data by group (fish species) over space and time: with only one value per group, a statistical analysis is a bit hard to be performed (no variation). Therefore, to the original CPUE values we add some random noise proportional to the average (by species, area, year):"
      ],
      "metadata": {
        "id": "XkfjMnGovB37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fish = fish.drop(['Unnamed: 0', 'Latitude', 'Longitude', 'Station'], axis=1)"
      ],
      "metadata": {
        "id": "gB-0jlk7ulWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## mutate variable\n",
        "fish['avg'] = fish.groupby(['Species', 'Area', 'Year'])['CPUE'].transform('mean')\n",
        "fish['std'] = 0.1 * fish['avg']"
      ],
      "metadata": {
        "id": "wGq_ZeqMMJ9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fish['noise'] = np.random.normal(loc=0, scale=fish['std'])\n",
        "fish['CPUE'] = fish['CPUE'] + fish['noise']"
      ],
      "metadata": {
        "id": "uUdU1vSxMRMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fish = fish.drop(['avg', 'std', 'noise'], axis=1)"
      ],
      "metadata": {
        "id": "hIqLK2raMaXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fish.head()"
      ],
      "metadata": {
        "id": "ptZUU48aMSfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(fish['CPUE'])\n",
        "X = np.array(fish[['Year','SST_cvW', 'SST_cvW5', 'SST_cvW4','SST_cvW3','SST_cvW2','SST_cvW1']])"
      ],
      "metadata": {
        "id": "daYxkR_yTX8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### One-hot encoding of categorical variables"
      ],
      "metadata": {
        "id": "7yGU6PZCe7Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = fish.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_columns"
      ],
      "metadata": {
        "id": "gnigwz0PYCTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(drop='first')"
      ],
      "metadata": {
        "id": "0VtLp6lQUnGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ohe = ohe.fit_transform(fish[categorical_columns]).toarray()\n",
        "#one_hot_array = encoder.fit_transform(df[['color']]).toarray()"
      ],
      "metadata": {
        "id": "pDsb1NRdXc9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ohe"
      ],
      "metadata": {
        "id": "wJfCKRvDY8A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ohe.shape"
      ],
      "metadata": {
        "id": "nQmEdh69aFFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.concatenate((X, X_ohe), axis=1)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "Ird0jYxmWCuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross validation: training/test split\n",
        "\n",
        "We start with a simple random split: 80% data for training, 10% test data"
      ],
      "metadata": {
        "id": "Bn_f56hlfAdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
      ],
      "metadata": {
        "id": "IpC3qAelMTWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "Vq3z1OVnesya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "OPVMJ0S3exVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data normalization\n",
        "\n",
        "We normalise the data using **standardization**: we want our numerical features to have zero mean and unit variance.\n",
        "\n",
        "First, we subset the trainig data by taking only the numerical features (first 7 columns). Please note that we are using the **training data for normalization**: this is important, since in real applications you don't have yet the test data."
      ],
      "metadata": {
        "id": "i3sZ9H_9jSmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ncols = X_train.shape[1]\n",
        "X_temp = X_train[:,0:7] ## the last index in the range is not included"
      ],
      "metadata": {
        "id": "kZMAX536jbGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.mean(axis=0)"
      ],
      "metadata": {
        "id": "AQ8pQkcWop8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_temp)"
      ],
      "metadata": {
        "id": "9elK_BIPZUW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled.mean(axis=0) ## all features have now zero mean"
      ],
      "metadata": {
        "id": "duYFreUQoi06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled.std(axis=0)"
      ],
      "metadata": {
        "id": "QCquM6RerxNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_scaled = np.concatenate((X_train_scaled, X_train[:,7:ncols]), axis=1)\n",
        "X_train_scaled.shape"
      ],
      "metadata": {
        "id": "9cDaf7QOk5iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_temp = X_test[:,0:7]\n",
        "X_test_scaled = scaler.transform(X_temp)\n",
        "X_test_scaled = np.concatenate((X_test_scaled, X_test[:,7:ncols]), axis=1)"
      ],
      "metadata": {
        "id": "Ob9OboInlc1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear regression model"
      ],
      "metadata": {
        "id": "A8ssbHQtjVm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression().fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "qr6yl_RXZafV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "id": "VDT_kCWNd3Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reg.score(X_train_scaled,y_train))"
      ],
      "metadata": {
        "id": "jYIPv5zsfN8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = reg.predict(X_test_scaled)\n",
        "\n",
        "plt.scatter(y_hat, y_test, alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DWo_e2J6dEQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.corrcoef(y_test,reg.predict(X_test_scaled))"
      ],
      "metadata": {
        "id": "AVRhb_8MctGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-fold cross validation\n",
        "\n",
        "We now implement k-fold cross-validation to measure the performance of a statistical (machine learning) model.\n",
        "\n",
        "The general scheme is depicted in the image below (from scikit-learn.org):\n",
        "\n",
        "!['k-fold-cv'](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
      ],
      "metadata": {
        "id": "nCOygZSQr_zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define a function that carries out the statistical model:\n",
        "\n",
        "- normalise the data\n",
        "- fit the linear model\n",
        "- evaluate the prformance of the model\n",
        "\n",
        "(For simplicity, we are normalising also OHE categorical variables)"
      ],
      "metadata": {
        "id": "z17o0cEzDZEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_model(feat_train, targ_train, feat_val, targ_val):\n",
        "\n",
        "  ## data normalization\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  feat_train_scaled = scaler.fit_transform(feat_train)\n",
        "  feat_val_scaled = scaler.transform(feat_val)\n",
        "\n",
        "  ## fit linear regression model\n",
        "  modfit = LinearRegression().fit(feat_train_scaled, targ_train)\n",
        "\n",
        "  ## model evaluation\n",
        "  y_hat = modfit.predict(feat_val_scaled)\n",
        "  pears_corr = np.corrcoef(targ_val, y_hat)[0,1]\n",
        "  mse = np.sum(((targ_val - y_hat)**2))/len(y_hat)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "  return((pears_corr, rmse))"
      ],
      "metadata": {
        "id": "ocpgjCOs3uKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# KFold split\n",
        "nsplits = 10\n",
        "kf = KFold(n_splits=nsplits)\n",
        "res = []\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "    print(f\"Fold {i}:\")\n",
        "\n",
        "    ## train and validatino sets\n",
        "    val_X = X[test_index,:]\n",
        "    val_y = y[test_index]\n",
        "\n",
        "    train_X = X[train_index,:]\n",
        "    train_y = y[train_index]\n",
        "\n",
        "    print(\"size of train set:\", len(train_y))\n",
        "    print(\"size of test set:\", len(val_y))\n",
        "\n",
        "    temp = train_test_model(train_X, train_y, val_X, val_y)\n",
        "    print(temp)\n",
        "\n",
        "    res.append(temp)\n"
      ],
      "metadata": {
        "id": "qYCM1qvHZfhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results have been stored in a list of tuples (Pearson correlation coefficient, RMSE):"
      ],
      "metadata": {
        "id": "hmTB73ekEWom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "S5O8dqE_Az_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the average correlation between predictions and observations in the test data:"
      ],
      "metadata": {
        "id": "DvE0NVNSEdqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_corr = np.mean([x[0] for x in res])\n",
        "print(\"Average correlation between predictions and observations is\", round(avg_corr, 3))"
      ],
      "metadata": {
        "id": "p7iFDpKqD86c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_rmse = np.mean([x[1] for x in res])\n",
        "print(\"Average RMSE of model predictions is\", round(avg_rmse, 3))"
      ],
      "metadata": {
        "id": "fSn8WbTrECch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}