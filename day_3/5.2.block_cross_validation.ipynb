{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Block cross validation\n",
        "\n",
        "After covering random cross-validation, we now introduce a more advanced topic: cross-validation for data with temporal, spatial, hierarchical or phylogenetic structure (stratified data).\n",
        "\n",
        "We are using the same dataset on fish catch."
      ],
      "metadata": {
        "id": "E-AJGZ6FqW_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "f6_nu99esFyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data\n",
        "\n",
        "Data from [Spatiotemporally explicit model averaging for forecasting of Alaskan groundfish catch](https://onlinelibrary.wiley.com/doi/10.1002/ece3.4488) (data repo [here](https://zenodo.org/record/4987796#.ZHcLL9JBxhE))\n",
        "\n",
        "It's data on fish catch (multiple fish species) over time in different regions of Alaska."
      ],
      "metadata": {
        "id": "-YUAvMpSr7Bf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hATDEqJTqOjC"
      },
      "outputs": [],
      "source": [
        "url= \"https://zenodo.org/records/4987796/files/stema_data.csv\"\n",
        "fish = pd.read_csv(url)\n",
        "fish.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing\n",
        "\n",
        "-   `V1` is record ID\n",
        "-   `Station` indicates the fishing station\n",
        "\n",
        "We will not consider these variables in the predictive model.\n",
        "\n",
        "In order to accommodate variation in SST among stations, the CPUE value has been replicated multiple times. This would defeat our purpose of analysing data by group (fish species) over space and time: with only one value per group, a statistical analysis is a bit hard to be performed (no variation). Therefore, to the original CPUE values we add some random noise proportional to the average (by species, area, year):"
      ],
      "metadata": {
        "id": "oJkEiaVrsBPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fish = fish.drop(['Unnamed: 0', 'Latitude', 'Longitude', 'Station'], axis=1)"
      ],
      "metadata": {
        "id": "7gJjzNJksBy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## mutate variable\n",
        "fish['avg'] = fish.groupby(['Species', 'Area', 'Year'])['CPUE'].transform('mean')\n",
        "fish['std'] = 0.1 * fish['avg']"
      ],
      "metadata": {
        "id": "4sojDKLzse7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fish['noise'] = np.random.normal(loc=0, scale=fish['std'])\n",
        "fish['CPUE'] = fish['CPUE'] + fish['noise']"
      ],
      "metadata": {
        "id": "SrWI3KlWshM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fish = fish.drop(['avg', 'std', 'noise'], axis=1)"
      ],
      "metadata": {
        "id": "ivLIId7jsjCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## sanity check!\n",
        "fish.head()"
      ],
      "metadata": {
        "id": "EMRlatDbsk6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Block validation strategies\n",
        "\n",
        "We first block by time (longitudinal data), using the variable `Year`:\n",
        "\n",
        "### 1. Define the data split\n",
        "\n",
        "We order data by Year: data are balanced, there are 292 records per year. The last 4 Years of data therefore represent 17.39% of the data"
      ],
      "metadata": {
        "id": "M0QG17t6ukO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fish['Year'].value_counts()"
      ],
      "metadata": {
        "id": "jVOzO5bsw5K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = fish.loc[fish['Year'] < 2009]\n",
        "test_set = fish.loc[fish['Year'] >= 2009]"
      ],
      "metadata": {
        "id": "T2nxm_ekxIwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A little sanity check on the data:"
      ],
      "metadata": {
        "id": "rkxwtJ02zlAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set['Year'].value_counts()"
      ],
      "metadata": {
        "id": "RzlhYzxezeaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set['Year'].value_counts()"
      ],
      "metadata": {
        "id": "rq3CSr58ziJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prepare the arrays for the linear model (we don't -and can't- use `Year` in the model now):"
      ],
      "metadata": {
        "id": "FoAto3n2swJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.array(train_set['CPUE'])\n",
        "X_train = np.array(train_set[['SST_cvW', 'SST_cvW5', 'SST_cvW4','SST_cvW3','SST_cvW2','SST_cvW1']])"
      ],
      "metadata": {
        "id": "LuU-6BO8ssBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data preprocessing\n",
        "\n",
        "#### One-hot encoding of categorical variables\n",
        "\n",
        "We now one-hot-encode categorical variables in the training set:"
      ],
      "metadata": {
        "id": "1KG1Wh8Jsu0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categorical_columns = train_set.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_columns = [x for x in categorical_columns if x != 'Year']\n",
        "ohe = OneHotEncoder(drop='first')\n",
        "X_train_ohe = ohe.fit_transform(train_set[categorical_columns]).toarray()\n",
        "X_train_ohe"
      ],
      "metadata": {
        "id": "qB2yOc_cs1gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate((X_train, X_train_ohe), axis=1)\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "fKWJg32Xs-CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data normalization\n",
        "\n",
        "We normalise the data using **standardization**: we want our numerical features to have zero mean and unit variance.\n",
        "\n",
        "First, we subset the trainig data by taking only the numerical features (first 7 columns). Please note that we are using the **training data for normalization**: this is important, since in real applications you don't have yet the test data."
      ],
      "metadata": {
        "id": "qSQIFvu-zbP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "ncols = X_train.shape[1]\n",
        "X_temp = X_train[:,0:6] ## the last index in the range is not included\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_temp)\n",
        "\n",
        "X_train_scaled = np.concatenate((X_train_scaled, X_train[:,6:ncols]), axis=1)\n",
        "X_train_scaled.shape"
      ],
      "metadata": {
        "id": "H2pmMqXUzvLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now the **test set**:"
      ],
      "metadata": {
        "id": "Az5JmEar0Hlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1) arrays\n",
        "y_test = np.array(test_set['CPUE'])\n",
        "X_test = np.array(test_set[['SST_cvW', 'SST_cvW5', 'SST_cvW4','SST_cvW3','SST_cvW2','SST_cvW1']])"
      ],
      "metadata": {
        "id": "zeX9PN3a0Ncq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2) OHE\n",
        "X_test_ohe = ohe.fit_transform(test_set[categorical_columns]).toarray()\n",
        "X_test_ohe"
      ],
      "metadata": {
        "id": "t-IfpCWR0Vnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.concatenate((X_test, X_test_ohe), axis=1)\n",
        "X_test.shape"
      ],
      "metadata": {
        "id": "ptryBysI0fuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3) normalization\n",
        "X_temp = X_test[:,0:6] ## the last index in the range is not included\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_temp)\n",
        "\n",
        "X_test_scaled = np.concatenate((X_test_scaled, X_test[:,6:ncols]), axis=1)\n",
        "X_test_scaled.shape"
      ],
      "metadata": {
        "id": "LtCmptOh0J9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Define and fit the model\n"
      ],
      "metadata": {
        "id": "nrQAusMFyQfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression().fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "-rKAuq-syQ77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(reg.score(X_train_scaled,y_train))"
      ],
      "metadata": {
        "id": "Xa6WFClxTGht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Get model predictions and evaluate the model\n",
        "\n",
        "We evaluate the model on the test data (new data: only the last 4 years):"
      ],
      "metadata": {
        "id": "wb3Jp4j7WqNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = reg.predict(X_test_scaled)\n",
        "dd = np.array([y_test, y_hat])\n",
        "dd = dd.T\n",
        "\n",
        "df = pd.DataFrame(dd, columns = ['y_test','y_hat'])"
      ],
      "metadata": {
        "id": "l9P-dKEZTIJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Species'] = np.array(test_set['Species'])\n",
        "df"
      ],
      "metadata": {
        "id": "dKXzWaFQVLdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import ggplot, aes, geom_point\n",
        "\n",
        "\n",
        "plot = (\n",
        "    ggplot(df, aes(x='y_hat', y='y_test')) +\n",
        "    geom_point(aes(color='Species'))\n",
        ")\n",
        "\n",
        "plot.draw()"
      ],
      "metadata": {
        "id": "zBvUre62TeQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['y_hat','y_test']].corr()"
      ],
      "metadata": {
        "id": "itQCWf5KU8EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = ((df['y_hat'] - df['y_test'])**2).sum()/(len(df))\n",
        "rmse = np.sqrt(mse)\n",
        "avg = df['y_test'].mean()\n",
        "nrmse = 100*(rmse/avg)"
      ],
      "metadata": {
        "id": "DeRts9pxVFfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The RMSE is \", round(rmse, 3))\n",
        "print(\"The NRMSE (normalised RMSE) is\", round(nrmse, 2), \"%\")"
      ],
      "metadata": {
        "id": "PJz_dWc4WORM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predictions by year?"
      ],
      "metadata": {
        "id": "ol1t-O9sXGJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "years = test_set['Year'].unique()\n",
        "df['Year'] = np.array(test_set['Year'])"
      ],
      "metadata": {
        "id": "XfzJP-CUW37C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "km6T6UD1XtG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = []\n",
        "for y in years:\n",
        "  print(\"processing year\",y)\n",
        "\n",
        "  temp = df.loc[df['Year'] == y]\n",
        "  lincorr = np.array(temp[['y_hat','y_test']].corr())[0,1]\n",
        "  mse = ((temp['y_hat'] - temp['y_test'])**2).sum()/(len(temp))\n",
        "  rmse = np.sqrt(mse)\n",
        "  avg = temp['y_test'].mean()\n",
        "  nrmse = 100*(rmse/avg)\n",
        "\n",
        "  #print(\"correlation:\", round(lincorr,3))\n",
        "  #print(\"RMSE:\", round(rmse,3))\n",
        "  #print(\"NRMSE:\", round(nrmse, 2), \"%\")\n",
        "\n",
        "  temp = {'year':y,'corr':lincorr, 'rmse':rmse, 'nrmse':nrmse}\n",
        "  res.append(temp)"
      ],
      "metadata": {
        "id": "ngP5mhfgXboR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_by_year = pd.DataFrame(res)\n",
        "res_by_year"
      ],
      "metadata": {
        "id": "IdqcIEDhOLCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EXERCISE: let's look at predictions by fish species"
      ],
      "metadata": {
        "id": "wBDyuFlQY435"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK 1: create an array or list of fish species"
      ],
      "metadata": {
        "id": "XGrocEwiXfXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK 2: add a column with fish species to the dataframe with predictions and observations on the test data"
      ],
      "metadata": {
        "id": "q6XGrCXLQAo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK 3: calculate average prediction metrics by fish species and store the results in a dataframe"
      ],
      "metadata": {
        "id": "hNwc1vxeQN3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK 4: visualize results (table; plot?)"
      ],
      "metadata": {
        "id": "3zb1uw-VQd91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Blocking by space"
      ],
      "metadata": {
        "id": "fC7JB-C5QikV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1) define the split\n",
        "areas = fish['Area'].unique()\n",
        "train_set = fish.loc[fish['Area'] != \"West Yakutat\"]\n",
        "test_set = fish.loc[fish['Area'] == \"West Yakutat\"]"
      ],
      "metadata": {
        "id": "1jLDv5t-QlTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2) OHE\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categorical_columns = train_set.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_columns = [x for x in categorical_columns if x != 'Area']\n",
        "ohe = OneHotEncoder(drop='first')\n",
        "\n",
        "## training set\n",
        "y_train = np.array(train_set['CPUE'])\n",
        "X_train = np.array(train_set[['SST_cvW', 'SST_cvW5', 'SST_cvW4','SST_cvW3','SST_cvW2','SST_cvW1']])\n",
        "X_train_ohe = ohe.fit_transform(train_set[categorical_columns]).toarray()\n",
        "X_train = np.concatenate((X_train, X_train_ohe), axis=1)\n",
        "\n",
        "## test set\n",
        "y_test = np.array(test_set['CPUE'])\n",
        "X_test = np.array(test_set[['SST_cvW', 'SST_cvW5', 'SST_cvW4','SST_cvW3','SST_cvW2','SST_cvW1']])\n",
        "X_test_ohe = ohe.fit_transform(test_set[categorical_columns]).toarray()\n",
        "X_test = np.concatenate((X_test, X_test_ohe), axis=1)\n",
        "\n",
        "\n",
        "print(\"training set size\", X_train.shape)\n",
        "print(\"test set size\", X_test.shape)"
      ],
      "metadata": {
        "id": "EWyPSawsWb-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3) normalization\n",
        "from sklearn import preprocessing\n",
        "\n",
        "## training set\n",
        "ncols = X_train.shape[1]\n",
        "X_temp = X_train[:,0:6] ## the last index in the range is not included\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_temp)\n",
        "X_train_scaled = np.concatenate((X_train_scaled, X_train[:,6:ncols]), axis=1)\n",
        "\n",
        "## test set\n",
        "X_temp = X_test[:,0:6] ## the last index in the range is not included\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X_test_scaled = scaler.fit_transform(X_temp)\n",
        "X_test_scaled = np.concatenate((X_test_scaled, X_test[:,6:ncols]), axis=1)"
      ],
      "metadata": {
        "id": "OUVLjU5vaMPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4) fit model and get predictions\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "## fit model\n",
        "reg = LinearRegression().fit(X_train_scaled, y_train)\n",
        "print(\"R^2 is\", reg.score(X_train_scaled,y_train))\n",
        "\n",
        "## predictions\n",
        "y_hat = reg.predict(X_test_scaled)\n",
        "dd = np.array([y_test, y_hat])\n",
        "dd = dd.T\n",
        "df = pd.DataFrame(dd, columns = ['y_test','y_hat'])\n",
        "df.shape"
      ],
      "metadata": {
        "id": "i5_v-LBeYWCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4) visualize results\n",
        "from plotnine import ggplot, aes, geom_point\n",
        "\n",
        "\n",
        "plot = (\n",
        "    ggplot(df, aes(x='y_hat', y='y_test')) +\n",
        "    geom_point()\n",
        ")\n",
        "\n",
        "plot.draw()"
      ],
      "metadata": {
        "id": "oirCIBZTdgAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lincorr = np.array(df[['y_hat','y_test']].corr().iloc[0,1])\n",
        "mse = ((df['y_hat'] - df['y_test'])**2).sum()/(len(df))\n",
        "rmse = np.sqrt(mse)\n",
        "avg = df['y_test'].mean()\n",
        "nrmse = 100*(rmse/avg)\n",
        "\n",
        "res = pd.DataFrame({'corr':[lincorr], 'rmse':rmse, 'nrmse':nrmse})\n",
        "res"
      ],
      "metadata": {
        "id": "iPh-xR5WeNQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Could we block by phylogeny/genetics with this dataset?**\n",
        "\n",
        "**Is it possible to block along more than one dimension?**"
      ],
      "metadata": {
        "id": "k-A3TS_GiEmW"
      }
    }
  ]
}