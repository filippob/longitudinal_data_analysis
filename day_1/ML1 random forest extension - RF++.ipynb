{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNA1E4eg1w9nZvBctpQeBQ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Setup"],"metadata":{"id":"nypL5stAjwLt"}},{"cell_type":"code","source":["# Reproducibility\n","import numpy as np\n","random_state = 777\n","np.random.seed(random_state)\n","\n","# Data description\n","n_subjects = 100\n","n_timepoints = 5 #still called time points, even if here the time has no effect\n","\n","# RF hyperparameters\n","n_estimators = 100\n","max_depth = 10\n","bootstrap_fraction = 0.6 #how many datapoint (or subjects) are given to each tree"],"metadata":{"id":"sdt-Wkakjk29","executionInfo":{"status":"ok","timestamp":1746025722039,"user_tz":-120,"elapsed":2,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Generate longitudinal dataset\n","\n","Two features (x1, x2), one target variable (y), `n_timepoints` repetitions for each sample."],"metadata":{"id":"OM-F10v5j2SU"}},{"cell_type":"code","source":["import pandas as pd\n","\n","data = []\n","\n","for subject_id in range(n_subjects):\n","    X1 = np.random.normal(5, 1)\n","    X2 = np.random.poisson(5)\n","    for time in range(n_timepoints):\n","        y = X1 + X2 + np.random.normal(0, 0.5)  # target depends on features plus noise, not time\n","        data.append({\n","            'subject_id': f'subject_{subject_id}',\n","            'X1': X1,\n","            'X2': X2,\n","            'y': y\n","        })\n","\n","df = pd.DataFrame(data)\n","\n","print(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9YREXIbkFQG","executionInfo":{"status":"ok","timestamp":1746025723914,"user_tz":-120,"elapsed":1832,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}},"outputId":"46e169fe-853c-45af-d0bd-ec8af72d1f46"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["     subject_id        X1  X2          y\n","0     subject_0  4.531791   6  10.120379\n","1     subject_0  4.531791   6  10.169630\n","2     subject_0  4.531791   6  10.660070\n","3     subject_0  4.531791   6  10.612921\n","4     subject_0  4.531791   6  10.151270\n","..          ...       ...  ..        ...\n","495  subject_99  5.712344  10  16.069661\n","496  subject_99  5.712344  10  15.353078\n","497  subject_99  5.712344  10  15.782522\n","498  subject_99  5.712344  10  15.213245\n","499  subject_99  5.712344  10  15.879670\n","\n","[500 rows x 4 columns]\n"]}]},{"cell_type":"markdown","source":["# Split train/validation sets"],"metadata":{"id":"mbkaZk1qlRWa"}},{"cell_type":"code","source":["# The list of subjects, to be shuffled\n","subjects = df['subject_id'].unique()\n","np.random.shuffle(subjects)\n","\n","# Separating the subjects in two lists with a classic 80/20 split\n","train_subjects = subjects[:80]   # 80 subjects for training\n","val_subjects = subjects[80:]     # 20 subjects for validation\n","\n","# Separating the actual data, so that info on the train subjects doesn't\n","# spill into the validation set\n","train_df = df[df['subject_id'].isin(train_subjects)]\n","val_df = df[df['subject_id'].isin(val_subjects)]\n","\n","# For easier interface, we also further split the data in X and y\n","X_train = train_df[['X1', 'X2']]\n","y_train = train_df['y']\n","subject_train = train_df['subject_id']\n","\n","X_val = val_df[['X1', 'X2']]\n","y_val = val_df['y']\n","subject_test = val_df['subject_id']"],"metadata":{"id":"dWkvbNyelbe7","executionInfo":{"status":"ok","timestamp":1746025723942,"user_tz":-120,"elapsed":22,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Standard Random Forest"],"metadata":{"id":"sax8UguUmLLQ"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# Instantiate the model\n","rf = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state, max_samples=bootstrap_fraction, max_depth=max_depth)\n","\n","# Training\n","rf.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred_rf = rf.predict(X_val)"],"metadata":{"id":"GIgXAPegmPhm","executionInfo":{"status":"ok","timestamp":1746025766149,"user_tz":-120,"elapsed":306,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"12799c53-833b-476d-c2c5-0491dde3a296"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["0    0.201619\n","1    0.798381\n","dtype: float64\n"]}]},{"cell_type":"markdown","source":["# RF++\n","\n","We'll need to implement the custom bootstrap described in the course: a number of subjects is selected and all their data points are given to a tree. We create a custom class `RandomForestPlusPlus` for the task."],"metadata":{"id":"mG-ZpLxHmeHm"}},{"cell_type":"code","source":["from sklearn.metrics import r2_score\n","from sklearn.tree import DecisionTreeRegressor\n","\n","class RandomForestPlusPlus:\n","    def __init__(self, n_estimators, max_depth, bootstrap_fraction, random_state):\n","        self.n_estimators = n_estimators\n","        self.max_depth = max_depth\n","        self.bootstrap_fraction = bootstrap_fraction\n","        self.random_state = np.random.RandomState(random_state)\n","        self.trees = []\n","\n","    def fit(self, X, y, subject_ids):\n","        self.trees = []\n","        Xy = X.copy()\n","        Xy['y'] = y\n","        Xy['subject_id'] = subject_ids\n","\n","        unique_subjects = subject_ids.unique()\n","\n","        for _ in range(self.n_estimators):\n","            target_size = int(self.bootstrap_fraction * len(unique_subjects))\n","            sampled_subjects = self.random_state.choice(unique_subjects, size=target_size, replace=False)\n","            sample = Xy[Xy['subject_id'].isin(sampled_subjects)]\n","            X_sample = sample.drop(columns=['y', 'subject_id'])\n","            y_sample = sample['y']\n","\n","            tree = DecisionTreeRegressor(max_depth=self.max_depth, random_state=self.random_state)\n","            tree.fit(X_sample, y_sample)\n","            self.trees.append(tree)\n","\n","    def predict(self, X):\n","        preds = np.zeros((len(self.trees), len(X)))\n","        for i, tree in enumerate(self.trees):\n","            preds[i] = tree.predict(X)\n","        return preds.mean(axis=0)"],"metadata":{"id":"FHeiUgdNiJi0","executionInfo":{"status":"ok","timestamp":1746025730326,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Train RF++, get the predictions\n","rfpp = RandomForestPlusPlus(n_estimators=n_estimators, max_depth=max_depth, bootstrap_fraction=bootstrap_fraction, random_state=random_state)\n","rfpp.fit(X_train, y_train, subject_train)\n","y_pred_rfpp = rfpp.predict(X_val)"],"metadata":{"id":"xVJ8ryEE7uYT","executionInfo":{"status":"ok","timestamp":1746025731652,"user_tz":-120,"elapsed":1317,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Comparing the two models"],"metadata":{"id":"Rx8NYfC-mhkL"}},{"cell_type":"code","source":["print(f\"Standard RF R2 score: {r2_score(y_val, y_pred_rf):.4f}\")\n","print(f\"RF++ R2 score: {r2_score(y_val, y_pred_rfpp):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_8nJK5Pmkl8","executionInfo":{"status":"ok","timestamp":1746025731676,"user_tz":-120,"elapsed":17,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}},"outputId":"9d61ce11-9794-4274-c5c8-de170c33f152"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Standard RF R2 score: 0.9285\n","RF++ R2 score: 0.9333\n"]}]}]}